---
alwaysApply: true
---
# Gemini FastAPI Toolkit - Cursor AI Rules

## Project Overview
FastAPI toolkit for Google Gemini API integration. Multimodal support for text, images, audio, video. Built for hackathon participants.

## Tech Stack
- FastAPI 0.109+, Python 3.10+
- google-generativeai, Pydantic v2
- aiohttp, redis, slowapi
- pytest, pytest-asyncio

## Code Style

### Always Use
- Type hints: `def func(x: str) -> dict:`
- Async/await: `async def func()`, `await api_call()`
- Pydantic models for requests/responses
- Dependency injection: `client: Client = Depends(get_client)`
- Structured logging with context
- Try-except with specific exceptions
- Docstrings (Google style)

### Never Use
- Synchronous I/O operations
- Bare `except:` clauses
- Hardcoded configuration
- Global mutable state
- `print()` statements (use logging)

## Project Structure
```
app/
├── main.py              # App init
├── config.py            # Settings
├── dependencies.py      # DI
├── core/                # Gemini client, exceptions
├── models/              # Pydantic schemas
├── routers/             # API endpoints
├── services/            # Business logic
└── utils/               # Helpers
```

## Patterns

### Router Template
```python
from fastapi import APIRouter, Depends, HTTPException, status
from app.models.text import TextRequest, TextResponse
from app.services.text_service import TextService

router = APIRouter(prefix="/api/v1/text", tags=["text"])

@router.post("/generate", response_model=TextResponse)
async def generate_text(
    request: TextRequest,
    service: TextService = Depends()
) -> TextResponse:
    """Generate text from prompt."""
    try:
        return await service.generate(request)
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### Pydantic Model Template
```python
from pydantic import BaseModel, Field
from typing import Optional

class TextRequest(BaseModel):
    prompt: str = Field(..., min_length=1, description="Input prompt")
    temperature: float = Field(0.7, ge=0.0, le=1.0)
    max_tokens: Optional[int] = Field(None, gt=0)
    
    class Config:
        json_schema_extra = {
            "example": {
                "prompt": "Hello world",
                "temperature": 0.7
            }
        }

class TextResponse(BaseModel):
    text: str
    model: str
    usage: dict
```

### Service Template
```python
from app.core.gemini_client import GeminiClient
import logging

logger = logging.getLogger(__name__)

class TextService:
    def __init__(self, client: GeminiClient = Depends()):
        self.client = client
    
    async def generate(self, request: TextRequest) -> TextResponse:
        try:
            result = await self.client.generate_text(
                request.prompt,
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            return TextResponse(
                text=result.text,
                model=result.model,
                usage=result.usage
            )
        except Exception as e:
            logger.error(f"Service error: {e}", exc_info=True)
            raise
```

### File Upload
```python
from fastapi import UploadFile, File
from app.utils.file_handler import validate_file

@router.post("/analyze")
async def analyze_image(
    file: UploadFile = File(...),
    prompt: str = "Describe this image"
):
    await validate_file(file, max_size_mb=10, allowed_types=["image/jpeg", "image/png"])
    contents = await file.read()
    # Process...
    await file.close()
```

### Streaming Response
```python
from fastapi.responses import StreamingResponse
from typing import AsyncGenerator

async def text_stream(prompt: str) -> AsyncGenerator[str, None]:
    async for chunk in client.generate_stream(prompt):
        yield f"data: {chunk}\n\n"

@router.post("/stream")
async def stream_text(request: TextRequest):
    return StreamingResponse(
        text_stream(request.prompt),
        media_type="text/event-stream"
    )
```

### Caching
```python
async def get_cached_or_generate(key: str, generator):
    cached = await cache.get(key)
    if cached:
        return cached
    
    result = await generator()
    await cache.set(key, result, ttl=3600)
    return result
```

### Error Handling
```python
from google.api_core.exceptions import GoogleAPIError, ResourceExhausted
from app.core.exceptions import GeminiAPIError

try:
    result = await client.generate_text(prompt)
except ResourceExhausted:
    raise HTTPException(status_code=429, detail="Rate limit exceeded")
except GoogleAPIError as e:
    logger.error(f"Gemini API error: {e}")
    raise HTTPException(status_code=503, detail="Service unavailable")
except Exception as e:
    logger.exception("Unexpected error")
    raise HTTPException(status_code=500, detail="Internal server error")
```

## Gemini API Patterns

### Initialization
```python
import google.generativeai as genai
from google.generativeai import GenerativeModel

genai.configure(api_key=api_key)
model = GenerativeModel('gemini-2.0-flash-exp')
```

### Text Generation
```python
response = await model.generate_content_async(
    prompt,
    generation_config=genai.GenerationConfig(
        temperature=0.7,
        max_output_tokens=1024,
        top_p=0.95,
        top_k=40
    )
)
text = response.text
```

### Image Analysis
```python
from PIL import Image
image = Image.open("file.jpg")
response = await model.generate_content_async([prompt, image])
```

### Streaming
```python
response = await model.generate_content_async(prompt, stream=True)
async for chunk in response:
    yield chunk.text
```

## Testing Pattern
```python
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_generate_text():
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.post(
            "/api/v1/text/generate",
            json={"prompt": "test", "temperature": 0.7}
        )
        assert response.status_code == 200
        assert "text" in response.json()

@pytest.fixture
def mock_gemini(monkeypatch):
    async def mock_generate(*args, **kwargs):
        return type('obj', (object,), {'text': 'mocked'})()
    monkeypatch.setattr("app.core.gemini_client.GeminiClient.generate_text", mock_generate)
```

## Import Order
1. Standard library
2. Third-party
3. Local (separate with blank line)

```python
import logging
from typing import Optional, List

from fastapi import APIRouter, Depends
from pydantic import BaseModel
import google.generativeai as genai

from app.core.gemini_client import GeminiClient
from app.models.text import TextRequest
```

## Naming
- Files: `text_service.py`
- Classes: `TextService`
- Functions: `generate_text()`
- Constants: `MAX_TOKENS`
- Private: `_internal_func()`

## Security Checklist
- Validate all inputs with Pydantic
- Never log API keys
- Sanitize error messages
- Validate file types and sizes
- Implement rate limiting
- Use HTTPS in production
- Set request timeouts
- Validate MIME types, not just extensions

## Performance
- Use `asyncio.gather()` for concurrent tasks
- Implement connection pooling
- Cache identical requests
- Use streaming for large responses
- Set appropriate timeouts
- Clean up resources (files, connections)

## Logging
```python
import logging

logger = logging.getLogger(__name__)

logger.info("Processing request", extra={
    "user_id": user_id,
    "model": model_name,
    "prompt_length": len(prompt)
})

logger.error("Failed to process", exc_info=True)
```

## Configuration
```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    GOOGLE_API_KEY: str
    GEMINI_MODEL: str = "gemini-2.0-flash-exp"
    
    class Config:
        env_file = ".env"

@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

## Common Tasks

### Add New Endpoint
1. Define models in `app/models/feature.py`
2. Create service in `app/services/feature_service.py`
3. Create router in `app/routers/feature.py`
4. Register in `app/main.py`: `app.include_router(feature.router)`
5. Write tests in `tests/test_feature.py`

### Add New Model
```python
class NewRequest(BaseModel):
    field: str = Field(..., description="Required field")
    optional: Optional[int] = Field(None, ge=0)
    
    class Config:
        json_schema_extra = {"example": {...}}
```

### Add Dependency
```python
# In app/dependencies.py
async def get_service(client: GeminiClient = Depends(get_gemini_client)):
    return MyService(client)

# Usage
service: MyService = Depends(get_service)
```

## Error Response Format
```python
{
    "detail": "Human-readable error message",
    "error_code": "SPECIFIC_ERROR_CODE",
    "timestamp": "2024-01-01T00:00:00Z"
}
```

## Documentation
- Use OpenAPI examples in Pydantic models
- Add docstrings to all public functions
- Include usage examples in docstrings
- Keep README.md updated
- Document environment variables

## Quick Checks Before Committing
- [ ] All functions have type hints
- [ ] All functions have docstrings
- [ ] Error handling is present
- [ ] Logging is added for important operations
- [ ] Tests are written and passing
- [ ] No hardcoded values
- [ ] No synchronous I/O
- [ ] Async/await used properly
- [ ] Pydantic validation on all inputs
- [ ] Resources are cleaned up

## Remember
- Async everywhere
- Type everything
- Validate all inputs
- Handle all errors
- Log important events
- Cache intelligently
- Test thoroughly
- Document clearly